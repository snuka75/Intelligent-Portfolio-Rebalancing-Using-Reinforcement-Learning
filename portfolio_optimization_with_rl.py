# -*- coding: utf-8 -*-
"""Portfolio Optimization with RL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q2gFsySzZIM5flEzwitNnnncwKMeiyDE
"""

# üì¶ Install required packages (run this in the first cell)
!pip install yfinance stable-baselines3 gym shimmy matplotlib

# üìä Import libraries
import yfinance as yf
import gym
import numpy as np
import pandas as pd
from gym import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
import matplotlib.pyplot as plt

# ‚úÖ Step 1: Load and clean stock data
tickers = ['AAPL', 'MSFT', 'GOOGL']
data = yf.download(tickers, start="2020-01-01", end="2023-01-01", auto_adjust=True)['Close']
data = data.dropna()
assert not data.isnull().values.any(), "Data contains NaNs!"

# ‚úÖ Step 2: Define the custom portfolio environment
class PortfolioEnv(gym.Env):
    def __init__(self, data):
        super(PortfolioEnv, self).__init__()
        self.data = data.reset_index(drop=True)
        self.n_assets = data.shape[1]
        self.current_step = 0
        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)
        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(self.n_assets,), dtype=np.float32)

    def reset(self):
        self.current_step = 0
        return self.data.iloc[self.current_step].values

    def step(self, action):
        self.current_step += 1
        done = self.current_step >= len(self.data) - 1

        # Prevent divide-by-zero
        if np.sum(action) == 0:
            weights = np.ones_like(action) / len(action)
        else:
            weights = action / np.sum(action)

        # Compute returns safely
        returns = self.data.pct_change().iloc[self.current_step].values
        returns = np.nan_to_num(returns, nan=0.0, posinf=0.0, neginf=0.0)

        portfolio_return = np.dot(weights, returns)
        reward = portfolio_return

        obs = self.data.iloc[self.current_step].values
        return obs, reward, done, {}

# ‚úÖ Step 3: Train PPO agent
env = PortfolioEnv(data)
vec_env = make_vec_env(lambda: env, n_envs=1)

model = PPO("MlpPolicy", vec_env, verbose=1)
model.learn(total_timesteps=10000)

# ‚úÖ Step 4: Evaluate the trained agent
obs = env.reset()
done = False
portfolio_value = 1.0
portfolio_history = [portfolio_value]

while not done:
    action, _ = model.predict(obs)
    obs, reward, done, _ = env.step(action)
    portfolio_value *= (1 + reward)
    portfolio_history.append(portfolio_value)

print(f"üìà Final portfolio value: {portfolio_value:.2f}")

# ‚úÖ Step 5: Plot results
plt.plot(portfolio_history)
plt.title("Portfolio Value Over Time")
plt.xlabel("Time Step")
plt.ylabel("Value")
plt.grid(True)
plt.show()

# üì¶ Install required packages
!pip install yfinance stable-baselines3 gym shimmy matplotlib ta --quiet

# üìä Imports
import yfinance as yf
import gym
import numpy as np
import pandas as pd
from gym import spaces
from collections import deque
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
import matplotlib.pyplot as plt

# ‚úÖ Load data
tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA', 'JPM', 'V', 'NFLX']
data = yf.download(tickers, start="2020-01-01", end="2023-01-01", auto_adjust=True)['Close']
data = data.dropna()
assert not data.isnull().values.any(), "Data contains NaNs!"

# ‚úÖ Custom Environment
class PortfolioEnv(gym.Env):
    def __init__(self, data):
        super(PortfolioEnv, self).__init__()
        self.data = data.reset_index(drop=True)
        self.n_assets = data.shape[1]
        self.current_step = 0
        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)
        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(self.n_assets,), dtype=np.float32)
        self.returns_window = deque(maxlen=30)
        self.prev_weights = np.ones(self.n_assets) / self.n_assets
        self.portfolio_value = 1.0

    def reset(self):
        self.current_step = 0
        self.returns_window.clear()
        self.prev_weights = np.ones(self.n_assets) / self.n_assets
        self.portfolio_value = 1.0
        return self.data.iloc[self.current_step].values

    def step(self, action):
        self.current_step += 1
        done = self.current_step >= len(self.data) - 1

        # Normalize and clip action
        action = np.clip(action, 1e-6, 1)
        weights = action / np.sum(action)

        # Use log returns for stability
        log_returns = np.log(self.data).diff().iloc[self.current_step].values
        log_returns = np.nan_to_num(log_returns, nan=0.0, posinf=0.0, neginf=0.0)

        portfolio_return = np.dot(weights, log_returns)
        self.returns_window.append(portfolio_return)

        # Sharpe Ratio as reward (for learning)
        if len(self.returns_window) > 1:
            reward = np.mean(self.returns_window) / (np.std(self.returns_window) + 1e-6)
        else:
            reward = portfolio_return

        reward = np.clip(reward, -10, 10)

        # Transaction cost penalty
        transaction_cost = np.sum(np.abs(weights - self.prev_weights)) * 0.002
        reward -= transaction_cost
        self.prev_weights = weights

        obs = self.data.iloc[self.current_step].values
        return obs, reward, done, {}

# ‚úÖ Train PPO agent
env = PortfolioEnv(data)
vec_env = make_vec_env(lambda: env, n_envs=1)

model = PPO("MlpPolicy", vec_env, verbose=1)
model.learn(total_timesteps=50000)

# ‚úÖ Evaluate the agent (using true returns, not reward)
obs = env.reset()
done = False
portfolio_value = 1.0
portfolio_history = [portfolio_value]

while not done:
    action, _ = model.predict(obs)
    obs, reward, done, _ = env.step(action)

    # Use actual return from last action (not reward)
    log_return = np.dot(env.prev_weights, np.log(env.data).diff().iloc[env.current_step].values)
    portfolio_value *= np.exp(log_return)
    portfolio_value = min(portfolio_value, 20)  # Optional cap
    portfolio_value = max(portfolio_value, 0.01)  # Prevent collapse
    portfolio_history.append(portfolio_value)

print(f"\nüìà Final portfolio value: {portfolio_value:.2f}")

# ‚úÖ Plot results
plt.figure(figsize=(10, 6))
plt.plot(portfolio_history, label='RL Portfolio')
plt.title("Portfolio Value Over Time")
plt.xlabel("Time Step")
plt.ylabel("Value")
plt.grid(True)
plt.legend()
plt.show()

# üì¶ Install required packages
!pip install yfinance stable-baselines3 gym shimmy matplotlib ta transformers feedparser --quiet

# üìä Imports
import yfinance as yf
import gym
import numpy as np
import pandas as pd
from gym import spaces
from collections import deque
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
import matplotlib.pyplot as plt
import feedparser
from transformers import pipeline

# ‚úÖ Load data
tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA', 'JPM', 'V', 'NFLX']
data = yf.download(tickers, start="2020-01-01", end="2023-01-01", auto_adjust=True)['Close']
data = data.dropna()
assert not data.isnull().values.any(), "Data contains NaNs!"

# ‚úÖ Load a lightweight sentiment pipeline
sentiment_pipeline = pipeline("sentiment-analysis", model="distilbert-base-uncased")

# ‚úÖ Real news source (Finviz RSS)
def get_finviz_headline():
    feed = feedparser.parse("https://finviz.com/feed.ashx")
    if feed.entries:
        return feed.entries[0].title
    return "No news available"

# ‚úÖ Custom Environment with Cached Sentiment Reward
class PortfolioEnv(gym.Env):
    def __init__(self, data):
        super(PortfolioEnv, self).__init__()
        self.data = data.reset_index(drop=True)
        self.n_assets = data.shape[1]
        self.current_step = 0
        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)
        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(self.n_assets,), dtype=np.float32)
        self.returns_window = deque(maxlen=30)
        self.prev_weights = np.ones(self.n_assets) / self.n_assets
        self.portfolio_value = 1.0
        self.sentiment_score = 0.0

    def reset(self):
        self.current_step = 0
        self.returns_window.clear()
        self.prev_weights = np.ones(self.n_assets) / self.n_assets
        self.portfolio_value = 1.0

        # Cache sentiment once per episode
        headline = get_finviz_headline()
        result = sentiment_pipeline(headline)[0]
        score = result['score'] if result['label'] == 'POSITIVE' else -result['score']
        self.sentiment_score = np.clip(score, -1, 1)

        return self.data.iloc[self.current_step].values

    def step(self, action):
        self.current_step += 1
        done = self.current_step >= len(self.data) - 1

        action = np.clip(action, 1e-6, 1)
        weights = action / np.sum(action)

        log_returns = np.log(self.data).diff().iloc[self.current_step].values
        log_returns = np.nan_to_num(log_returns, nan=0.0, posinf=0.0, neginf=0.0)
        portfolio_return = np.dot(weights, log_returns)
        self.returns_window.append(portfolio_return)

        if len(self.returns_window) > 1:
            base_reward = np.mean(self.returns_window) / (np.std(self.returns_window) + 1e-6)
        else:
            base_reward = portfolio_return
        base_reward = np.clip(base_reward, -5, 5)

        reward = base_reward + 0.1 * self.sentiment_score

        transaction_cost = np.sum(np.abs(weights - self.prev_weights)) * 0.002
        reward -= transaction_cost
        reward = np.clip(reward, -5, 5)

        self.prev_weights = weights
        obs = self.data.iloc[self.current_step].values
        return obs, reward, done, {}

# ‚úÖ Train PPO agent (short run for testing)
env = PortfolioEnv(data)
vec_env = make_vec_env(lambda: env, n_envs=1)

model = PPO("MlpPolicy", vec_env, verbose=1)
model.learn(total_timesteps=5000)

# ‚úÖ Evaluate the agent (track allocations and value)
obs = env.reset()
done = False
portfolio_value = 1.0
portfolio_history = [portfolio_value]
allocation_history = []

while not done:
    action, _ = model.predict(obs)
    action = np.clip(action, 1e-6, 1)
    weights = action / np.sum(action)
    allocation_history.append(weights)

    obs, reward, done, _ = env.step(action)

    log_return = np.dot(env.prev_weights, np.log(env.data).diff().iloc[env.current_step].values)
    portfolio_value *= np.exp(log_return)
    portfolio_value = min(portfolio_value, 20)
    portfolio_value = max(portfolio_value, 0.01)
    portfolio_history.append(portfolio_value)

print(f"\nüìà Final portfolio value: {portfolio_value:.2f}")

# ‚úÖ Plot portfolio value
plt.figure(figsize=(10, 6))
plt.plot(portfolio_history, label='RL Portfolio')
plt.title("Portfolio Value Over Time")
plt.xlabel("Time Step")
plt.ylabel("Value")
plt.grid(True)
plt.legend()
plt.show()


import seaborn as sns

plt.figure(figsize=(12, 6))
sns.heatmap(alloc_df.T, cmap='YlGnBu', cbar_kws={'label': 'Allocation %'})
plt.title("Asset Allocation Heatmap (Stocks vs Time)")
plt.xlabel("Time Step")
plt.ylabel("Stocks")
plt.tight_layout()
plt.show()

# üì¶ Install required packages
!pip install yfinance stable-baselines3 gym shimmy matplotlib ta transformers feedparser --quiet

# üìä Imports
import yfinance as yf
import gym
import numpy as np
import pandas as pd
from gym import spaces
from collections import deque
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
import matplotlib.pyplot as plt
import feedparser
from transformers import pipeline

# ‚úÖ Load data
tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA', 'JPM', 'V', 'NFLX']
data = yf.download(tickers + ['SPY'], start="2020-01-01", end="2023-01-01", auto_adjust=True)['Close']
data = data.dropna()
assert not data.isnull().values.any(), "Data contains NaNs!"

# ‚úÖ Load a lightweight sentiment pipeline
sentiment_pipeline = pipeline("sentiment-analysis", model="distilbert-base-uncased")

# ‚úÖ Real news source (Finviz RSS)
def get_finviz_headline():
    feed = feedparser.parse("https://finviz.com/feed.ashx")
    if feed.entries:
        return feed.entries[0].title
    return "No news available"

# ‚úÖ Custom Environment with Cached Sentiment Reward
class PortfolioEnv(gym.Env):
    def __init__(self, data):
        super(PortfolioEnv, self).__init__()
        self.data = data.reset_index(drop=True)[tickers]  # drop SPY column for RL
        self.n_assets = self.data.shape[1]
        self.current_step = 0
        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)
        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(self.n_assets,), dtype=np.float32)
        self.returns_window = deque(maxlen=30)
        self.prev_weights = np.ones(self.n_assets) / self.n_assets
        self.portfolio_value = 1.0
        self.sentiment_score = 0.0

    def reset(self):
        self.current_step = 0
        self.returns_window.clear()
        self.prev_weights = np.ones(self.n_assets) / self.n_assets
        self.portfolio_value = 1.0

        headline = get_finviz_headline()
        result = sentiment_pipeline(headline)[0]
        score = result['score'] if result['label'] == 'POSITIVE' else -result['score']
        self.sentiment_score = np.clip(score, -1, 1)

        return self.data.iloc[self.current_step].values

    def step(self, action):
        self.current_step += 1
        done = self.current_step >= len(self.data) - 1

        action = np.clip(action, 1e-6, 1)
        weights = action / np.sum(action)

        log_returns = np.log(self.data).diff().iloc[self.current_step].values
        log_returns = np.nan_to_num(log_returns, nan=0.0, posinf=0.0, neginf=0.0)
        portfolio_return = np.dot(weights, log_returns)
        self.returns_window.append(portfolio_return)

        if len(self.returns_window) > 1:
            base_reward = np.mean(self.returns_window) / (np.std(self.returns_window) + 1e-6)
        else:
            base_reward = portfolio_return
        base_reward = np.clip(base_reward, -5, 5)

        reward = base_reward + 0.1 * self.sentiment_score

        transaction_cost = np.sum(np.abs(weights - self.prev_weights)) * 0.002
        reward -= transaction_cost
        reward = np.clip(reward, -5, 5)

        self.prev_weights = weights
        obs = self.data.iloc[self.current_step].values
        return obs, reward, done, {}

# ‚úÖ Train PPO agent (short run for testing)
env = PortfolioEnv(data)
vec_env = make_vec_env(lambda: env, n_envs=1)

model = PPO("MlpPolicy", vec_env, verbose=1)
model.learn(total_timesteps=5000)

# ‚úÖ Evaluate the agent
obs = env.reset()
done = False
portfolio_value = 1.0
portfolio_history = [portfolio_value]
allocation_history = []

while not done:
    action, _ = model.predict(obs)
    action = np.clip(action, 1e-6, 1)
    weights = action / np.sum(action)
    allocation_history.append(weights)
    obs, reward, done, _ = env.step(action)

    log_return = np.dot(env.prev_weights, np.log(env.data).diff().iloc[env.current_step].values)
    portfolio_value *= np.exp(log_return)
    portfolio_value = min(portfolio_value, 20)
    portfolio_value = max(portfolio_value, 0.01)
    portfolio_history.append(portfolio_value)

print(f"\nüìà Final portfolio value: {portfolio_value:.2f}")

# ‚úÖ Plot portfolio value
plt.figure(figsize=(10, 6))
plt.plot(portfolio_history, label='RL Portfolio')

# ‚úÖ Add benchmark (SPY)
spy_prices = data['SPY'].values
spy_returns = np.log(spy_prices[1:] / spy_prices[:-1])
spy_cumulative = [1.0]
for r in spy_returns:
    spy_cumulative.append(spy_cumulative[-1] * np.exp(r))
plt.plot(spy_cumulative[:len(portfolio_history)], label='SPY (Benchmark)')

plt.title("Portfolio Value vs SPY Benchmark")
plt.xlabel("Time Step")
plt.ylabel("Value")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

# ‚úÖ Plot asset allocations
alloc_df = pd.DataFrame(allocation_history, columns=tickers)
alloc_df.plot.area(figsize=(14, 6), alpha=0.8)
plt.title("RL Agent Portfolio Allocation Over Time")
plt.xlabel("Time Step")
plt.ylabel("Allocation (%)")
plt.grid(True)
plt.legend(loc='upper left', bbox_to_anchor=(1.02, 1), title="Stocks")
plt.tight_layout()
plt.show()

# üì¶ Install required packages
!pip install yfinance stable-baselines3 gym shimmy matplotlib ta transformers feedparser --quiet

# üìä Imports
import yfinance as yf
import gym
import numpy as np
import pandas as pd
from gym import spaces
from collections import deque
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
import matplotlib.pyplot as plt
import feedparser
from transformers import pipeline

# ‚úÖ Load data
tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA', 'JPM', 'V', 'NFLX']
data = yf.download(tickers + ['SPY'], start="2020-01-01", end="2023-01-01", auto_adjust=True)['Close']
data = data.dropna()
assert not data.isnull().values.any(), "Data contains NaNs!"

# ‚úÖ Load a lightweight sentiment pipeline
sentiment_pipeline = pipeline("sentiment-analysis", model="distilbert-base-uncased")

# ‚úÖ Real news source (Finviz RSS)
def get_finviz_headline():
    feed = feedparser.parse("https://finviz.com/feed.ashx")
    if feed.entries:
        return feed.entries[0].title
    return "No news available"

# ‚úÖ Custom Environment with Cached Sentiment Reward
class PortfolioEnv(gym.Env):
    def __init__(self, data):
        super(PortfolioEnv, self).__init__()
        self.data = data.reset_index(drop=True)[tickers]
        self.n_assets = self.data.shape[1]
        self.current_step = 0
        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)
        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(self.n_assets,), dtype=np.float32)
        self.returns_window = deque(maxlen=30)
        self.prev_weights = np.ones(self.n_assets) / self.n_assets
        self.portfolio_value = 1.0
        self.sentiment_score = 0.0
        self.reward_log = []  # üß† Log reward components

    def reset(self):
        self.current_step = 0
        self.returns_window.clear()
        self.prev_weights = np.ones(self.n_assets) / self.n_assets
        self.portfolio_value = 1.0

        headline = get_finviz_headline()
        result = sentiment_pipeline(headline)[0]
        score = result['score'] if result['label'] == 'POSITIVE' else -result['score']
        self.sentiment_score = np.clip(score, -1, 1)

        self.reward_log = []
        return self.data.iloc[self.current_step].values

    def step(self, action):
        self.current_step += 1
        done = self.current_step >= len(self.data) - 1

        action = np.clip(action, 1e-6, 1)
        weights = action / np.sum(action)

        log_returns = np.log(self.data).diff().iloc[self.current_step].values
        log_returns = np.nan_to_num(log_returns, nan=0.0, posinf=0.0, neginf=0.0)
        portfolio_return = np.dot(weights, log_returns)
        self.returns_window.append(portfolio_return)

        if len(self.returns_window) > 1:
            base_reward = np.mean(self.returns_window) / (np.std(self.returns_window) + 1e-6)
        else:
            base_reward = portfolio_return
        base_reward = np.clip(base_reward, -5, 5)

        sentiment_bonus = 0.1 * self.sentiment_score
        transaction_cost = np.sum(np.abs(weights - self.prev_weights)) * 0.002
        reward = base_reward + sentiment_bonus - transaction_cost
        reward = np.clip(reward, -5, 5)

        # Log reward components
        self.reward_log.append({
            "step": self.current_step,
            "return": portfolio_return,
            "base_reward": base_reward,
            "sentiment": self.sentiment_score,
            "transaction_cost": transaction_cost,
            "final_reward": reward
        })

        self.prev_weights = weights
        obs = self.data.iloc[self.current_step].values
        return obs, reward, done, {}

# ‚úÖ Train PPO agent
env = PortfolioEnv(data)
vec_env = make_vec_env(lambda: env, n_envs=1)
model = PPO("MlpPolicy", vec_env, verbose=1)
model.learn(total_timesteps=5000)

# ‚úÖ Evaluate the agent
obs = env.reset()
done = False
portfolio_value = 1.0
portfolio_history = [portfolio_value]
allocation_history = []

while not done:
    action, _ = model.predict(obs)
    action = np.clip(action, 1e-6, 1)
    weights = action / np.sum(action)
    allocation_history.append(weights)
    obs, reward, done, _ = env.step(action)

    log_return = np.dot(env.prev_weights, np.log(env.data).diff().iloc[env.current_step].values)
    portfolio_value *= np.exp(log_return)
    portfolio_value = min(portfolio_value, 20)
    portfolio_value = max(portfolio_value, 0.01)
    portfolio_history.append(portfolio_value)

print(f"\nüìà Final portfolio value: {portfolio_value:.2f}")

# ‚úÖ Benchmark SPY cumulative return
spy_prices = data['SPY'].values
spy_returns = np.log(spy_prices[1:] / spy_prices[:-1])
spy_cumulative = [1.0]
for r in spy_returns:
    spy_cumulative.append(spy_cumulative[-1] * np.exp(r))

# ‚úÖ Performance Metrics
rl_return = portfolio_history[-1] / portfolio_history[0] - 1
spy_return = spy_cumulative[len(portfolio_history)-1] / spy_cumulative[0] - 1
rl_vol = np.std(np.diff(np.log(portfolio_history))) * np.sqrt(252)
spy_vol = np.std(np.diff(np.log(spy_cumulative[:len(portfolio_history)]))) * np.sqrt(252)
rl_sharpe = rl_return / rl_vol
spy_sharpe = spy_return / spy_vol

print("\nüîç Performance Comparison:")
print(f"RL Portfolio - Return: {rl_return:.2%}, Volatility: {rl_vol:.2%}, Sharpe: {rl_sharpe:.2f}")
print(f"SPY Benchmark - Return: {spy_return:.2%}, Volatility: {spy_vol:.2%}, Sharpe: {spy_sharpe:.2f}")

# ‚úÖ Plot portfolio value
plt.figure(figsize=(10, 6))
plt.plot(portfolio_history, label='RL Portfolio')
plt.plot(spy_cumulative[:len(portfolio_history)], label='SPY (Benchmark)')
plt.title("Portfolio Value vs SPY Benchmark")
plt.xlabel("Time Step")
plt.ylabel("Value")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

# ‚úÖ Plot asset allocations
alloc_df = pd.DataFrame(allocation_history, columns=tickers)
alloc_df.plot.area(figsize=(14, 6), alpha=0.8)
plt.title("RL Agent Portfolio Allocation Over Time")
plt.xlabel("Time Step")
plt.ylabel("Allocation (%)")
plt.grid(True)
plt.legend(loc='upper left', bbox_to_anchor=(1.02, 1), title="Stocks")
plt.tight_layout()
plt.show()

# ‚úÖ Plot reward breakdown
reward_df = pd.DataFrame(env.reward_log)
plt.figure(figsize=(14, 6))
plt.plot(reward_df["step"], reward_df["final_reward"], label="Final Reward", linewidth=2)
plt.plot(reward_df["step"], reward_df["base_reward"], label="Sharpe/Base Reward", linestyle='--')
plt.plot(reward_df["step"], reward_df["sentiment"] * 0.1, label="Sentiment Bonus", linestyle=':')
plt.plot(reward_df["step"], -reward_df["transaction_cost"], label="Transaction Cost", linestyle='-.')
plt.title("Reward Breakdown Over Time")
plt.xlabel("Time Step")
plt.ylabel("Reward Value")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()